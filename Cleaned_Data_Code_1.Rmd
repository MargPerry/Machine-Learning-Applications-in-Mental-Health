---
title: "New Dataset"
author: "Hanxiao Lu"
date: "11/30/2018"
output: html_document
---
#WHAT ARE LEFT TO DO: BINNING some variables; generate trees; turn predict responses into 1 or 0 
```{r, results='hide'}
library(tidyr)
library(dplyr)
library(mosaic)
library(tree)
library(ISLR)
library(ggplot2)
library(class)
library(foreign)
library(readr)
```

#Cleaned data
```{r}
newdata= read.csv('newdata.csv')
dat.25 <- newdata[, -which(colMeans(is.na(newdata)) > 0.25)]
```

#converting the response variable into either 1 or 0 and remove observations with missing values 
```{r}
dat.25$X <- NULL
dat.25$V07657 <- NULL
dat.25$V07655 <- as.numeric(dat.25$V07655)
dat.25$V07655[dat.25$V07655 == "2"] <- 0 
dat.25$V07876 <- as.numeric(dat.25$V07876)
dat.25$V07876[dat.25$V07876 == "2"] <- 0 
dat.25 <- dat.25 %>%
  filter(complete.cases(.))
```

#because the orignial ratio of depressed/not depressed is very small, we need to restore the balance by adding one more condition: whether the person has any serious suicidal thoughts. Filter out the observations in which the answer is YES. 
```{r}
dat.25 <- dat.25 %>%
  filter(dat.25$V01993 == "YES")
dat.25$V01993 <- NULL
```

#binning 
```{r}
#turning V09389 mother's years of education into bins 
#
```

#turning some into numeric? can we indicate religious preference in numbers? 
```{r}

```

#turning logi into factor
```{r}
dat.25$V03221 <- as.factor(dat.25$V03221 )
dat.25$V03223 <- as.factor(dat.25$V03223 )
dat.25$V03224 <- as.factor(dat.25$V03224 )
dat.25$V03225 <- as.factor(dat.25$V03225 )
dat.25$V03226 <- as.factor(dat.25$V03226 )
dat.25$V03227 <- as.factor(dat.25$V03227 )
dat.25$V03228 <- as.factor(dat.25$V03228 )
dat.25$V03229 <- as.factor(dat.25$V03229 )
dat.25$V03230 <- as.factor(dat.25$V03230 )
dat.25$V03231 <- as.factor(dat.25$V03231 )
dat.25$V09389 <- as.factor(dat.25$V09389)
dat.25$V07655 <- as.factor(dat.25$V07655 )
dat.25$V07876 <- as.factor(dat.25$V07876 )
```

#in previous tests, suicidal thoughts is very closely associated with the endorsement of depression
```{r}
endorsed.dep <- dat.25 %>%
  filter(dat.25$V07655 == "1")
#so we have 319 out of 1587 are endorsed with depression with 12 month episode: 20%. Slightly better balance of observations in two levels. 

notendorsed <- dat.25 %>%
  filter(dat.25$V07655 == "0")

notendorsed = notendorsed[sample(nrow(notendorsed), 319), ]

newtrain = rbind(endorsed.dep, notendorsed)

set.seed(3)
train = newtrain%>%
  sample_frac(.5)
test = newtrain%>%
  setdiff(train)

tree4 <- tree(V07655 ~. -V07876, data=train)
summary(tree4)
plot(tree4)
text(tree4, pretty = 0)

```

```{r}
tree_pred = predict(tree4, test, type = "class")
table(tree_pred, test$V07655)
```

#Classficiation Tree model for 12month episode 
```{r}
set.seed(1)
train = dat.25%>%
  sample_frac(.5)
test = dat.25%>%
  setdiff(train)

tree1 <- tree(V07655 ~. -V07876, data=train)
summary(tree1)
plot(tree1)
text(tree1, pretty = 0)

tree2 <- tree(V07876 ~. -V07655, data=train)
summary(tree2)
plot(tree2)
text(tree2, pretty = 0)
```

```{r}
tree_pred = predict(tree1, test, type = "class")
table(tree_pred, test$V07655)
```

So it seems like our tree model would not classify any observations into "endorsed". And the accuration rate is $630/(630+163) = 79.5%$. Maybe it finds that it is the easiest and safest to not classify anyone as depressed...

#boosting 
```{r}
library(gbm)

set.seed(8)
train = dat.25%>%
  sample_frac(.5)
test = dat.25%>%
  setdiff(train)

boost.tree <- gbm(V07655~.- V07876, data = train, distribution = "bernoulli", n.trees = 5000, interaction.depth = 3)
summary(boost.tree)

```


```{r}
boost_estimate = predict(boost.tree,
newdata = test,
n.trees= 5000,
type="response")
```

```{r}
histogram(boost_estimate)
```






