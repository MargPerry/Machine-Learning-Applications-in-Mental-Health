---
title: "New Dataset"
author: "Hanxiao Lu"
date: "11/30/2018"
output: html_document
--- 

##things to do: labeling the variables?
```{r, results='hide'}
library(tidyr)
library(dplyr)
library(mosaic)
library(tree)
library(ISLR)
library(ggplot2)
library(class)
library(foreign)
library(readr)
library(gbm)
```

#Cleaned data
```{r}
newdata= read.csv('newdata.csv')
dat.25 <- newdata[, -which(colMeans(is.na(newdata)) > 0.25)]
```

#converting the response variable into either 1 or 0 and remove observations with missing values 
```{r}
dat.25$X <- NULL
dat.25$V07657 <- NULL
dat.25$V07655 <- as.numeric(dat.25$V07655)
dat.25$V07655[dat.25$V07655 == "2"] <- 0 
dat.25$V07876 <- as.numeric(dat.25$V07876)
dat.25$V07876[dat.25$V07876 == "2"] <- 0 
dat.25 <- dat.25 %>%
  filter(complete.cases(.))
```

#because the orignial ratio of depressed/not depressed is very small, we need to restore the balance by adding one more condition: whether the person has any serious suicidal thoughts. Filter out the observations in which the answer is YES. 
```{r}
dat.25 <- dat.25 %>%
  filter(dat.25$V01993 == "YES")
dat.25$V01993 <- NULL
```

#boosting 
```{r}
dat.25$V03221 <- as.factor(dat.25$V03221 )
dat.25$V03223 <- as.factor(dat.25$V03223 )
dat.25$V03224 <- as.factor(dat.25$V03224 )
dat.25$V03225 <- as.factor(dat.25$V03225 )
dat.25$V03226 <- as.factor(dat.25$V03226 )
dat.25$V03227 <- as.factor(dat.25$V03227 )
dat.25$V03228 <- as.factor(dat.25$V03228 )
dat.25$V03229 <- as.factor(dat.25$V03229 )
dat.25$V03230 <- as.factor(dat.25$V03230 )
dat.25$V03231 <- as.factor(dat.25$V03231 )
set.seed(3)
train = dat.25%>%
  sample_frac(.5)
test = dat.25%>%
  setdiff(train)

boost.tree = gbm(V07655~. - V07876, data = train, distribution = "bernoulli", n.trees = 5000, shrinkage = 0.01)
summary(boost.tree)

```

Here we can see that V09389, V09414, and V03085 are three very important variables. We can also produce partial dependence plots for these two variables. These plots illustrate the marginal effect of the selected variables on the response after integrating out the other variables. 

Use the boosting model to predict the response on the test data. Predict that a person will make a purchase if the estimated probability of purchase is greater than 20 %. Form a confusion matrix. What fraction of the people predicted to make a purchase do in fact make one? How does this compare with the results obtained from applying KNN or logistic regression to this data set?

```{r}
boost.pred = predict(boost.tree, test, type = "response", n.trees=5000)
#The boost predict is not either yes or no, so we need to reclassify them
boost.pred.class = ifelse(boost.pred > 0.2, 1,0)
table(boost.pred.class, test$V07655, dnn = c("pred", "test"))
```

```{r}
histogram(boost.pred)
```

$(396+75)/(794) =  0.593199$ so we have around 59% accuracy. 

#turning logi into factor
```{r}
dat.25$V03221 <- as.factor(dat.25$V03221 )
dat.25$V03223 <- as.factor(dat.25$V03223 )
dat.25$V03224 <- as.factor(dat.25$V03224 )
dat.25$V03225 <- as.factor(dat.25$V03225 )
dat.25$V03226 <- as.factor(dat.25$V03226 )
dat.25$V03227 <- as.factor(dat.25$V03227 )
dat.25$V03228 <- as.factor(dat.25$V03228 )
dat.25$V03229 <- as.factor(dat.25$V03229 )
dat.25$V03230 <- as.factor(dat.25$V03230 )
dat.25$V03231 <- as.factor(dat.25$V03231 )
dat.25$V09389 <- as.factor(dat.25$V09389)
dat.25$V07655 <- as.factor(dat.25$V07655 )
dat.25$V07876 <- as.factor(dat.25$V07876 )
```

#in previous tests, suicidal thoughts is very closely associated with the endorsement of depression
```{r}
endorsed.dep <- dat.25 %>%
  filter(dat.25$V07655 == "1")
#so we have 319 out of 1587 are endorsed with depression with 12 month episode: 20%. Slightly better balance of observations in two levels. 

notendorsed <- dat.25 %>%
  filter(dat.25$V07655 == "0")

notendorsed = notendorsed[sample(nrow(notendorsed), 319), ]

newtrain = rbind(endorsed.dep, notendorsed)

set.seed(3)
train = newtrain%>%
  sample_frac(.5)
test = newtrain%>%
  setdiff(train)

tree4 <- tree(V07655 ~. -V07876, data=train)
summary(tree4)
plot(tree4)
text(tree4, pretty = 0)

```

```{r}
tree_pred = predict(tree4, test, type = "class")
table(tree_pred, test$V07655)
```

#Classficiation Tree model for 12month episode 
```{r}
set.seed(1)
train = dat.25%>%
  sample_frac(.5)
test = dat.25%>%
  setdiff(train)

#12 month episode 
tree1 <- tree(V07655 ~. -V07876, data=train)
summary(tree1)
plot(tree1)
text(tree1, pretty = 0)

#lifetime episode
tree2 <- tree(V07876 ~. -V07655, data=train)
summary(tree2)
plot(tree2)
text(tree2, pretty = 0)
```

```{r}
tree_pred = predict(tree1, test, type = "class")
table(tree_pred, test$V07655)

tree_pred = predict(tree2, test, type = "class")
table(tree_pred, test$V07655)
```

So it seems like our tree model would not classify any observations into "endorsed". And the accuration rate is $630/(630+163) = 79.5%$. Maybe it finds that it is the easiest and safest to not classify anyone as depressed...







